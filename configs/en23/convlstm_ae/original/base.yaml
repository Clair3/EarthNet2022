
Architecture: "convlstm_ae"

Seed: 42

#Dataset setting
Setting: "en23"

Logger:
    save_dir: "experiments/"

Checkpointer:
    save_top_k: 1
    save_last: True
    every_n_epochs: 5


Trainer:
    # The "auto" option recognizes the machine you are on, and selects the respective Accelerator.
    accelerator: "auto" # "gpu" 
    # Accumulates grads every k batches
    accumulate_grad_batches: 1 #8
    # Automatically tries to find the largest batch size that fits into memory, before any training.
    auto_scale_batch_size: "binsearch"
    # pick available GPUs automatically
    auto_select_gpus: False
    # Find the best lr using tune
    auto_lr_find: True
    # Accelerate the training (if the input size doesn't change)
    benchmark: False
    # Ensures reproducibility
    deterministic: True
    # Number of devices to train on (int) or which devices to train on (list or str)
    devices: [2, 3]
    check_val_every_n_epoch: 1
    # Gradient clipping value. In case of gradient exploding, clip the gradient norm to the defined value.  
    gradient_clip_val: 0 # default setting, no clipping gradient. 
    # How often to add logging rows
    log_every_n_steps: 32
    # Sanity check runs n batches of val before starting the training routine.
    num_sanity_val_steps: 2
    # Number of GPU nodes for distributed training.
    num_nodes: 1
    max_epochs: 50
    precision: 16
    profiler: True #"Advanced"
    # Training with the DistributedDataParallel strategy
    strategy: 'ddp'
    # Help to identify vanishing or exploding gradient by tracking the n-norm (e.g. 2-norm)
    track_grad_norm: 2
    # How often within one training epoch to check the validation set.
    val_check_interval: 1 # default setting
    # Whether to use torch.inference_mode() or torch.no_grad() mode during evaluation
    # inference_mode: True # if False use 'torch.no_grad' instead. 
  
Data:
    base_dir: "/workspace/data/africa_v0.3/" 
    test_track: "iid"
    target: "ndvi"
    # With a batch of size N, a DPP strategy with D devices, and accumulate_grad_batches = K, the effective batch size = K*D*N. 
    # With a DP strategy, the effective batch size= K*N.
    train_batch_size: 4 
    val_batch_size: 4
    test_batch_size: 4 
    num_workers: 6 # Common bottleneck during the training, increase until saturation. 

Task:
    loss:
        name: "masked" # loss computed with the non-vegetation pixels masked.
        args: {
            distance_type: "L2"
        }
        dist_scale: 1
        min_lc: 2
        max_lc: 6
    context_length: 35
    target_length: 10
    # for variationnal models
    n_stochastic_preds: 1 
    optimization:
        optimizer:
            - 
                name: Adam
                args: 
                    betas: [0.9, 0.999]
                lr_per_sample: 0.0000001
        lr_shedule:
            -
                name: MultiStepLR
                args:
                    milestones: [1000] #[2, 20, 50, 90]
                    gamma: 0.1
    n_log_batches: 8
    compute_metric_on_test: True

Model:
    hidden_dim: [64, 64, 64, 64]
    kernel_size: 3
    num_layers: 3
    bias: True
    skip_connections: True